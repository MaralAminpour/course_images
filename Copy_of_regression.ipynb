{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Lf43r_2OMlkCcGUYp_Rl1Pw29v4oBp5d",
      "authorship_tag": "ABX9TyNNrIO3SpIi69uiSPdBl6Eo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MaralAminpour/course_images/blob/main/Copy_of_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AI vs Machine Learning vs Deep Learning**"
      ],
      "metadata": {
        "id": "rKPiQhQSGpxm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Artificial Intelligence is the broader umbrella under which Machine Learning and Deep Learning come. And you can also see in the diagram that even deep learning is a subset of Machine Learning. So all three of them AI, machine learning and deep learning are just the subsets of each other. So let us move on and understand how exactly they are different from each other.\n",
        "\n",
        "\"Artificial Intelligence is a technique that allows machines to act like humans by replicating their behavior and nature.\"\n",
        "\n",
        "“Machine Learning is a subset of artificial intelligence. It allows the machines to learn and make predictions based on its experience(data)“\n",
        "\n",
        "“Deep learning is a particular kind of machine learning that achieves great power and flexibility by learning to represent the world as nested hierarchy of concepts or abstraction”\n"
      ],
      "metadata": {
        "id": "iEaiRH7wUURu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " <img src='https://drive.google.com/uc?export=view&id=1zIanfMFeZWX5qdPXs0xjU5fgS9NTAK52' width=600px>"
      ],
      "metadata": {
        "id": "t_Y50zYujaQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is Machine learning?**\n",
        "\n",
        "You started with some data sets, some data. And you fed it to some magical machine learning algorithm and derive intelligence.  \n",
        "\n",
        " <img src='https://drive.google.com/uc?export=view&id=1Y0pYeUZsLXKo7KXMQ0qE-ARaLKynzDLA' width=600px>\n",
        "\n",
        "**The Machine Learning Process**\n",
        "\n",
        "- Define the problem\n",
        "This can be difficult. What are we trying to achieve? What are we trying to predict?\n",
        "- Get the data\n",
        "\n",
        "This is often connected with the problem definition step, because knowing about the data helps clarify what we can do with it\n",
        "- Prepare data\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1UgC6BfIbkozV-jgyBufw6cFqoF4jEZwu' width=500px align=\"right\">\n",
        "\n",
        "Exploratory data analysis and visualization\n",
        "- Cleaning data\n",
        "\n",
        "Often the most tedious and time consuming step\n",
        "- Select Algorithm\n",
        "\n",
        "Setting up one or more machine learning pipelines\n",
        "- Train the model\n",
        "\n",
        "Feed the algorithm data.\n",
        "\n",
        "- Test the mode\n",
        "\n",
        "Maybe we need to go back and select a different algorithm to work with?\n",
        "\n",
        "- Select the best model\n",
        "\n",
        "The definition of \"best\" depends on the type of problem, the type of data, and\n",
        "\n",
        "- our goals\n",
        "\n",
        "Predict\n",
        "Use the model to make predictions based on unseen data\n",
        "$$$ (?)"
      ],
      "metadata": {
        "id": "F-9dPGKB7tCb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " <img src='https://drive.google.com/uc?export=view&id=1BWr7GRnSxreVAyWBNUWtQt5fY0_ZNeKX' width=200px>\n"
      ],
      "metadata": {
        "id": "vdVa_T8hVUd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Traditional modeling vs machine learning**\n",
        "\n",
        "You can think of traditional scientific modeling like cooking with a recipe. We know exactly what ingredients go in, and we can predict what will come out, just like we use equations to predict results in our models.\n",
        "\n",
        "On the other hand, machine learning is more like a magical cooking pot. We understand the basic idea - that the pot learns from what we've cooked before to make new dishes. But the exciting part is, we don't always know exactly what the pot will whip up next, or why it chose to add a pinch of this or a sprinkle of that (this is often called a 'black box model').\n",
        "\n",
        "Traditional science uses 'white box' models, where we know exactly how the inputs become outputs, like using a set formula. On the other hand, machine learning is more of a 'black box' model. We know the machine is learning, but we don't always understand the hows and whys of what it learns. This mystery is sometimes seen as a drawback of machine learning.\n",
        "\n",
        "Some folks might not like the mystery pot method, arguing that not fully understanding the process could be a problem. But hey, it's also part of what makes machine learning so intriguing!"
      ],
      "metadata": {
        "id": "gjsAwYOG1Lko"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Course Overview**\n",
        "\n",
        "Hey there! In this facinating journey, we'll be exploring machine learning as seen through a variety of case studies across diverse BME applications. This approach will help to solidify the fundamental principles at work. Rather than diving straight into the complicated details of algorithms and methods, we'll take a more practical approach that closely mirrors real-world scenarios.\n",
        "\n",
        "Here's the thing: if we dig into algorithms right off the bat, we often end up with overly simplistic use cases that don't really reflect what you'll encounter in reality. Therefore, we're going to shake things up in this course. Our first step will be to consider real-life use cases. This hands-on strategy will give you\n",
        "\n",
        "- a deeper understanding of the core concepts and\n",
        "- techniques required to build intelligent applications,\n",
        "- gauge their performance or quality, and determine whether they're functioning optimally.\n",
        "\n",
        "By the end of this course, you'll have a handful of these intelligent applications under your belt! To create such applications, we'll need to consider several questions: What task am I aiming to accomplish? Is it a classification problem? What machine learning models should I use, such as support vector machines or regression? How can I optimize the parameters of my chosen model? Lastly, is the system delivering the level of intelligence I'm aiming for and how can I quantify its quality?\n",
        "\n",
        "In this course, we'll delve into the nitty-gritty of model description and optimization in subsequent courses. Our initial focus, however, will be on determining the task at hand, identifying appropriate machine learning techniques, and understanding how to measure their effectiveness. We'll primarily use algorithms as 'black boxes', allowing us to construct a variety of genuinely intelligent and exciting applications together.\n",
        "\n",
        "Don't worry, we'll actually get our hands dirty by coding and building these applications in diverse ways. As we progress through this course, which includes 11 weeks and a capstone project, we'll dive deeper into different areas. Here's a sneak peek of the depth and variety we'll be exploring throughout this exciting course.\n",
        "\n",
        "**NOTE: you need to change this according to our course**\n",
        "\n",
        "So the regression course is going to talk about various models of predicting a real value, so for example, a house price from the features of the house. And we're going to discuss linear regression techniques, we're going to discuss advanced techniques like ridge regression and lasso that allow you to select what features are most appropriate for your problem. We're going to talk about optimization techniques like gradient descent and coordinate descent to optimize the parameters of those models. As well as some key machine learning concepts like loss functions, bias-variance tradeoffs, cross-validation. Things that you need to know to really take this method and kind of improve them, develop them and build applications with them."
      ],
      "metadata": {
        "id": "-es3Kt4J7Vs7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predicting house prices: A case study in regression**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PrOBJ2_p3J2V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One of the classic examples used to teach regression is house pricing. In this context, the \"intelligence\" we're seeking to derive is the potential value of a house that's not currently on the market. Our goal is to estimate this unknown value using data.\n",
        "\n",
        "So, where does this data originate? For this case, we'll be studying other houses, using their sale prices as a basis to infer the potential value of our target house. But the sale price isn't the only factor we're considering. We'll also look at various characteristics of these houses, such as the number of bedrooms, bathrooms, and the total square footage, among other things.\n",
        "\n",
        "Our machine learning strategy will be to establish a relationship between these house features and their sale prices. By successfully modeling this relationship with our data from past house sales, we can then use it to predict the sale price of a new house. In simpler terms, we'll be taking the new house's attributes to forecast its potential sale price. This practical technique is known as regression."
      ],
      "metadata": {
        "id": "LV9F0F_lRN4W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " <img src='https://drive.google.com/uc?export=view&id=1uIHJsWM4l5mWtsuaFnn-CcTCkv0wD3O_' width=600px align=\"right\">\n",
        "\n",
        "\n",
        " Data: Prices of houses\n",
        "\n",
        "and other features\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pC4Qy-IsrV4H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " <img src='https://drive.google.com/uc?export=view&id=1usPRRGU4wGPa65lXFhVSwb9rkgxWIiCx' width=150px align=\"right\">\n",
        "\n",
        " sale price of a new house"
      ],
      "metadata": {
        "id": "MGBawoWis4Lz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Suggested videos and tutorials about predicting house prices**\n",
        "\n",
        "1. Simple Linear Regression by Emily Fox (16 minutes)\n",
        "\n",
        "check this video about gouse porice prediction and linear regression [here ](https://youtu.be/xsTmUtwUg9Q)\n",
        "\n",
        "2. check the related tutorial by Smith White [here](https://colab.research.google.com/github/ualberta-rcg/python-machine-learning/blob/main/notebooks/02-regression.ipynb)."
      ],
      "metadata": {
        "id": "TqvzIwaHqxok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The Titanic Kaggle challenge machine learning: A case study for classification**\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1jbTFiD4AHpp7zqom8MWc86PVDglbhsWI' width=500px align=\"center\">\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9EqIiL3W-jEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's explore machine learning with a fun example from Kaggle, a competition site owned by Google. These contests can be for giggles, cash prizes, or even a job offer sometimes!\n",
        "\n",
        "One beginner's challenge is based on the Titanic disaster, a famous shipwreck that happened on April 15, 1912. The ship, thought to be unsinkable, hit an iceberg and sank, sadly causing 1502 out of 2224 people onboard to lose their lives because there weren't enough lifeboats.\n",
        "\n",
        "Here's the interesting part: it seems that some people were more likely to survive than others. The challenge asks us to figure out who these folks were, using data like their names, ages, genders, and social classes.\n",
        "\n",
        "We get a file with details about 891 passengers, including whether they survived or not. We'll use this data to teach our machine to make smart guesses.\n",
        "\n",
        "But the real test comes with another file, this one has information on 418 passengers, but doesn't tell us if they survived. That's where our machine's predictions come in!\n",
        "\n",
        "Suggested tutorial about Kaggle's titanic challange: [Titanic - Machine Learning from Disaster](https://www.kaggle.com/competitions/titanic)\n",
        "\n",
        "You can follow Chris White's tutorial on this subject through this Jupyter notebook: [01-intro-classification.ipynb](https://colab.research.google.com/github/ualberta-rcg/python-machine-learning/blob/main/notebooks/01-intro-classification.ipynb#scrollTo=Gd80ekh_zQu4)."
      ],
      "metadata": {
        "id": "Q7NOAsZB_7yw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One-hot encoding**\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1r-2ngkSjI7aues9ILuih25kby705yzLx' width=500px align=\"center\">\n",
        "\n",
        "Machine learning is a bit like a kid who only likes to play with numbers and not with words. So, when we have categories like **'apple' and 'peer'**, we need to find a way to turn them into numbers.\n",
        "\n",
        "This is where one-hot encoding comes in handy! It's a cool trick that turns these categories into something machine learning algorithms can work with. It's like giving each category its own 'on' and 'off' switch.\n",
        "\n",
        "And guess what? There's an easy way to do this if you're using pandas, a tool in Python. It has a function called 'get_dummies' that does all the work for you.\n",
        "\n",
        "**I thought it would be intresting for you to know**: the term \"one-hot\" in \"one-hot encoding\" comes from the way digital circuits are designed. In digital electronics, a one-hot signal is a group of bits among which the legal combinations of values are only those with a single high bit (1) and all the others low (0)."
      ],
      "metadata": {
        "id": "SK1wS_yBCwQn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = pd.get_dummies(train_df[features])\n",
        "X"
      ],
      "metadata": {
        "id": "0LrgglXZG7sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The 'get_dummies' function is pretty cool. It only changes the columns that need changing and leaves the ones with numbers just the way they are.\n",
        "\n",
        "By the way, even though **passenger class **is expressed as a number, have you ever wondered if we should treat it as a category instead? Just a thought!"
      ],
      "metadata": {
        "id": "JtSPgugeF3ts"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're now going to split our data into two groups. One group is like our study material that we'll use to teach our machine. The other group is like a pop quiz to see how well the machine learned from the study material. The machine won't have seen this quiz data during its 'study' time.\n",
        "\n",
        "Here's how we do it: we'll use two-thirds of our data for teaching (or 'training') and keep one-third for the quiz (or 'testing'). We want to see if our machine's guesses on the quiz match the actual answers.\n",
        "\n",
        "There's a handy tool in scikit-learn, a library in Python, called 'train_test_split' that does the splitting for us.\n",
        "\n",
        "Here's a little code snippet:"
      ],
      "metadata": {
        "id": "gRksDHhjGkm-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
      ],
      "metadata": {
        "id": "K6EpaxTjG0F6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's break down what this does:\n",
        "\n",
        "- X_train is the data we use for training (two-thirds of the total data)\n",
        "- y_train is the actual answers for the training data\n",
        "- X_test is the quiz questions (the remaining one-third of the data)\n",
        "- y_test is the real answers for the quiz. We'll use these to check how well our machine did."
      ],
      "metadata": {
        "id": "WTE-J5W_Go6R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Our first machine learning model: Decision Tree**\n",
        "\n",
        "Let's kick off our machine learning adventure with a model called a decision tree. Imagine it like a game of 20 questions. The model asks questions about the data and makes decisions based on the answers. And the cool thing? It can change or fine-tune its answers as it gets more information!\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1pd3ojKWIOaPmMzKMaZE06MQg1rDv3-g7' width=500px align=\"center\">\n",
        "\n",
        "(Image courtsey: Audrey Fukman and Andy Wright on SFoodie, via Serious Eats)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5hnjcYK_HCzZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*sklearn.tree.DecisionTreeClassifier*\n",
        "\n",
        "Think of sklearn's DecisionTreeClassifier as a super-smart detective. It checks out your data and figures out all the right questions to ask on its own automatically. We can even tell it how deep we want it to dig into the data with an option called 'max_depth'(basically, how deep do we want the questions to go).\n"
      ],
      "metadata": {
        "id": "9Fd3-KLHIuD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "model = DecisionTreeClassifier(max_depth=3)"
      ],
      "metadata": {
        "id": "tWcPj40gJxtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### `fit`\n",
        "\n",
        "Most, if not all, models from Scikit-learn have a handy tool called a 'fit' method. It's like a personal trainer for your model, helping it learn from your data features and labels.\n",
        "Think of it as the model's learning button—it uses your features and labels to teach the model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y3jIToX_JzbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit doesn't modify the model in place\n",
        "# (returns a trained model)\n",
        "\n",
        "model = model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "0F0y6uN2LP_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### `predict`\n",
        "\n",
        "Almost every model in Scikit-learn comes with a neat feature called a 'predict' method. Once your model has been trained, 'predict' lets it make guesses about new data that doesn't have labels.\n",
        "\n",
        "Here we use our **test data** (data that the model hasn't seen before) as an input"
      ],
      "metadata": {
        "id": "N5IQKQUxLSZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(X_test)"
      ],
      "metadata": {
        "id": "BAmhKfgaMRFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The first ten predicted labels for the test data\")\n",
        "print(list(predictions[:10]))\n",
        "print('The ten actual labels')\n",
        "print(list(y_test[:10]))"
      ],
      "metadata": {
        "id": "8OA5lfN8MSBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###But what did the decision tree do?"
      ],
      "metadata": {
        "id": "UkOWUj_4Mam9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you want to install graphviz ....\n",
        "# Note for conda, you may have to install both graphviz and python-graphviz\n",
        "# !pip install graphviz\n",
        "# !conda install graphviz python-graphviz"
      ],
      "metadata": {
        "id": "3rcRvHs1Mils"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import graphviz\n",
        "from sklearn.tree import export_graphviz\n",
        "from graphviz import Source\n",
        "\n",
        "dot_data = export_graphviz(model,\n",
        "                           feature_names=X.columns,\n",
        "                           class_names=['Died', 'Survived'],\n",
        "                           filled=True, rounded=True,\n",
        "                           special_characters=True,\n",
        "                           out_file=None)\n",
        "graph = graphviz.Source(dot_data)\n",
        "graph"
      ],
      "metadata": {
        "id": "vqKulYCtMjjz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Measuring the quality of predictions in classification models"
      ],
      "metadata": {
        "id": "oFlAQ4_PM3sO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright, it's time to get into the world of stats and technical terms! We're going to explore something called a **'confusion matrix**'. This is a handy tool that allows us to compare our model's predictions with the actual results in our test data."
      ],
      "metadata": {
        "id": "hhkaW33dNPlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(y_test, predictions)"
      ],
      "metadata": {
        "id": "Sj__jz6uNa6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A confusion matrix is a bit like a report card for our predictions. It gives us the numbers for when we got things right and when we got them wrong, in a handy little table:\n",
        "\n",
        "|          | Predicted 0         | Predicted 1         |\n",
        "|----------|---------------------|---------------------|\n",
        "| **Actual 0** | True negative (TN)  | False positive (FP) |\n",
        "| **Actual 1** | False negative (FN) | True positive (TP)  |\n",
        "\n",
        "Predicted Didn't Survive\tPredicted Survived\n",
        "Actually Didn't Survive\tTrue negative (TN)\tFalse positive (FP)\n",
        "Actually Survived\tFalse negative (FN)\tTrue positive (TP)\n",
        "The true negatives and true positives are when our predictions match reality. True negatives are when we said a person wouldn't survive, and they didn't. True positives are when we said a person would survive, and they did.\n",
        "\n",
        "False negatives and false positives are where we slipped up. False negatives are when we said a person wouldn't survive, but they ended up surviving. False positives are when we said a person would survive, but they didn't.\n",
        "\n",
        "We can calculate accuracy as the percentage of times we got it right. It's the number of true negatives and true positives divided by the total number of predictions. We can do this in Scikit-learn using the accuracy_score function. Here's how:"
      ],
      "metadata": {
        "id": "y2qJyaXFOAQt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider a team of medical researchers trying to predict whether or not a patient is at risk of heart failure:\n",
        "Just like in our previous examples, accuracy is the proportion of correct predictions. But, as we noted earlier, if heart failures are rare, a model that always predicts \"no heart failure\" might have a high accuracy but it's not really helpful in the medical context. So, it's important to consider more than just accuracy when evaluating such predictions!"
      ],
      "metadata": {
        "id": "yTLl9nPWOrcL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1os_zF6e9JpaxsP8Yw-jWTrQCDFUW6TTH' width=500px align=\"center\">\n"
      ],
      "metadata": {
        "id": "DfhfsOogPokr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy is basically how often our model gets its guesses right. It's calculated by adding up all the times our model correctly predicted heart failure (true positives) and correctly predicted no heart failure (true negatives), then dividing that by the total number of predictions. In other words,\n",
        "\n",
        "accuracy = (TN + TP) / (TN + TP + FN + FP).\n",
        "\n",
        "In Scikit-learn, we can calculate this using something called 'accuracy_score'."
      ],
      "metadata": {
        "id": "JWTD8U77QV0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy_score(predictions, y_test)"
      ],
      "metadata": {
        "id": "zFQ9GhOyQZ6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy might not always give us the full picture. Let's say we want to predict a rare disease that only affects 1 in every 1,000 people. If we create a model that always predicts that no one has the disease, it will be 99.9% accurate, but it's not very useful, right?\n",
        "\n",
        "The same can be applied to our **Titanic challenge**. If we made a model that simply predicted everyone on board died, it would be 62% accurate. But that model wouldn't tell us much about who actually had a better chance of survival. So, accuracy isn't always the best judge of a model's performance!"
      ],
      "metadata": {
        "id": "Cd3uZlHYQeJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Precision and recall might not be as straightforward as accuracy, but in certain cases, they can give us a better understanding of how our model is performing."
      ],
      "metadata": {
        "id": "RY6Jiy1PQ8rR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine a circle that represents all the patients we predicted would experience heart failure. Now, the left part of that circle represents the patients who actually did experience heart failure.\n",
        "\n",
        "**Precision** is like asking,\n",
        "\n",
        "\"*Out of all the patients we predicted would experience heart failure, how many actually did?*\" We calculate it as:\n",
        "\n",
        "TP / (TP + FP).\n",
        "\n",
        "**Recall**, on the other hand, is like asking,\n",
        "\n",
        "\"*Out of all the patients who actually experienced heart failure, how many did we correctly predict?*\" We calculate it as:\n",
        "\n",
        "TP / (TP + FN).\n",
        "\n",
        "Now, there are times when precision or recall matters more:\n",
        "\n",
        "**High precision** is key when we really want to avoid false positives.\n",
        "\n",
        "- For example, in court trials, we want to be sure that if we declare someone guilty, they really are.\n",
        "\n",
        "- Another example is email spam filters. We don't want to accidentally label important emails as spam.\n",
        "\n",
        "**High recall** is critical when we want to avoid false negatives.\n",
        "\n",
        "- For example, in cancer screenings, a false negative could mean a patient who actually has cancer gets a clean bill of health.\n",
        "\n",
        "Think about it, what do you think is more important for recommendation engines like YouTube, Netflix, or Spotify? Would it be precision or recall?"
      ],
      "metadata": {
        "id": "1rLPsoFrR2X7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scores for our model ..."
      ],
      "metadata": {
        "id": "5cBuMtDFSypQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "print('Accuracy: {}'.format(accuracy_score(y_test, predictions)))\n",
        "print('Precision: {}'.format(precision_score(y_test, predictions)))\n",
        "print('Recall: {}'.format(recall_score(y_test, predictions)))"
      ],
      "metadata": {
        "id": "JLx_iRVsSz4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bringing it all together...**\n",
        "\n",
        "We've been working on different pieces in separate parts of this notebook. Let's now bring all of that code together. This way, we can see the big picture of our entire process more clearly."
      ],
      "metadata": {
        "id": "OgijolYOTBaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "\n",
        "# More about this line shortly ...\n",
        "# np.random.seed(1337)\n",
        "\n",
        "# Load data\n",
        "train_df = pd.read_csv('data/titanic/train.csv')\n",
        "\n",
        "# Choose features and lables\n",
        "features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\n",
        "X = pd.get_dummies(train_df[features], drop_first=True)\n",
        "y = train_df['Survived']\n",
        "\n",
        "# Split data into training and test data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
        "\n",
        "# Initialize model and fit to training data\n",
        "model = DecisionTreeClassifier(max_depth=3)\n",
        "model = model.fit(X_train, y_train)\n",
        "\n",
        "# Use model to predict on unseen test data\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Evaluate how well the model did\n",
        "print('Accuracy: {}'.format(accuracy_score(y_test, predictions)))\n",
        "print('Precision: {}'.format(precision_score(y_test, predictions)))\n",
        "print('Recall: {}'.format(recall_score(y_test, predictions)))"
      ],
      "metadata": {
        "id": "ypqUHCVhTDa4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Repeatability ...**\n",
        "\n",
        "Try running the previous section a few times. Notice anything different each time? A lot of machine learning code relies on random number generators, which can make it tricky to get the same results every time.\n",
        "\n",
        "Scikit-learn, the toolkit we're using, relies on a random number generator from a library called NumPy. Fortunately, we can set a starting point for this random number generator (think of it like setting a starting line for the random numbers). This makes sure that we get the same sequence of random numbers every time.\n",
        "\n",
        "See that line in the code that reads: np.random.seed(1337)? If you remove the '#' sign in front of it, you can run the code several times and get the same results each time.\n",
        "\n",
        "Don't worry about the number 1337 - it's just a common choice because in internet slang it stands for 'leet' or 'elite'. Feel free to choose any number that makes you smile! Just remember to use it every time for the same process if you want to get repeatable results."
      ],
      "metadata": {
        "id": "AdSFRCBdTlYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**If a tree is good, is a forest better?**\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1_eGAlGv9sC4qGGJvnd63PFjjRLF6FoDk' width=300px align=\"center\">\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=14PFkayLkgMuf8PiuXNb4CEeSAz4Hlznb' width=400px align=\"right\">\n",
        "\n",
        "Think about it this way - if one tree is handy, wouldn't a whole forest be even better?\n",
        "\n",
        "Imagine we could create a bunch of decision trees, each one asking different questions. Then, we could combine their answers to make a final prediction. This is exactly what a Random Forest does.\n",
        "\n",
        "A Random Forest is what we call an 'ensemble model'. It's like a supergroup band made up of lots of individual musicians, all working together to create a harmonious sound. Here, each model contributes to a final, hopefully better, prediction.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SBSEBCOaUOrB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our toolkit, Scikit-learn, there's a way to use Random Forest. It's called *'sklearn.ensemble.RandomForestClassifier'*.\n",
        "\n",
        "Just like with DecisionTree, we can decide how deep we want the decision trees to go using 'max_depth'.\n",
        "\n",
        "But there's another cool setting we can adjust. It's called 'n_estimators', and it lets us decide how many trees to use in our forest. For example, if we want to use 100 trees and set our max_depth to 3, we'd write:\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=100, max_depth=3). Pretty neat, right?"
      ],
      "metadata": {
        "id": "0nOIIQgMY_5a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exercise: use a RandomForestClassifier\n",
        "\n",
        "Try your hand at setting up a machine learning pipeline using a RandomForestClassifier model. Follow these steps:\n",
        "\n",
        "- Start by grabbing the **Titanic data** from the file.\n",
        "- Divide the data into two sections: training data and testing data.\n",
        "- Next, fit a random forest model using the training data.\n",
        "- Then, use the test data to make predictions.\n",
        "- Finally, see how your model did by evaluating its performance.\n",
        "\n",
        "Give it a shot! It's a great way to practice what you've learned."
      ],
      "metadata": {
        "id": "0QBwyKlSZRgq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Your code goes here ...\n",
        "# Hint: other than a couple of lines of code, it should look\n",
        "#   very much like the decision tree pipeline above"
      ],
      "metadata": {
        "id": "gK-lH4fUZqTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRINT SOLUTION (copy/paste output into a cell to run)\n",
        "show_solution('titanic-random-forest-pipeline.py')"
      ],
      "metadata": {
        "id": "f366qawiZsj4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handling missing data**\n",
        "Alright, let's take another peek at the details of our data set:"
      ],
      "metadata": {
        "id": "Y6xI5S5WZ-71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.info()"
      ],
      "metadata": {
        "id": "Cvt-WkM2aMGe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine if we believed that age could really help us predict survival from heart failure. However, we hit a bit of a snag:\n",
        "\n",
        "Some of our records don't include data for age. This is a problem because most machine learning algorithms get stumped when they encounter missing data."
      ],
      "metadata": {
        "id": "5nrXvbT4aMm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, there are a few ways we can tackle this problem:\n",
        "\n",
        "We could simply get rid of any rows with missing data.\n",
        "Alternatively, we could fill in the blanks with a default value. This could be the average value, if available, or something like zero, -1, or 9999.\n",
        "Or, perhaps the absence of a value could be informative in itself? For instance, we could record a one if there's a cabin number for the patient, and a zero otherwise.\n",
        "Now, let's explore how to discard rows and how to replace missing values with the average."
      ],
      "metadata": {
        "id": "jLdPMycpaqB9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Removing Rows**\n",
        "\n",
        "Firstly, let's use a handy tool to identify where we have missing (or null) values:"
      ],
      "metadata": {
        "id": "fb34n5g1auf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.isnull()"
      ],
      "metadata": {
        "id": "p_Q9y3J8a3Ll"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the missing data in each column\n",
        "train_df.isnull().sum()"
      ],
      "metadata": {
        "id": "SVxAe4VCa5rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pandas dataframes have this useful feature called 'dropna' that can come to the rescue. It helps us filter out rows that have missing data. We can even specify which columns we want to focus on using a special keyword called 'subset'. The best part? It doesn't mess with our original dataframe. Instead, it gives us a fresh new dataframe, all tidy and missing-data-free!"
      ],
      "metadata": {
        "id": "soQX2uxmbY-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "age_non_null_train_df = train_df.dropna(subset=['Age'])\n",
        "age_non_null_train_df.info()\n",
        "\n",
        "# If we decide to go further down this road, we might do either:\n",
        "#    train_df.dropna(subset=['Age'], inplace=True)\n",
        "#                or\n",
        "#    train_df = train_df.dropna(subset=['Age'])"
      ],
      "metadata": {
        "id": "55kcTWkJbeP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Replacing missing data with a mean**\n",
        "\n",
        "Here's another approach: Instead of tossing out rows, we can get creative and fill in the missing age values with a \"fictional\" age. Let's calculate the mean age from the available non-null values:"
      ],
      "metadata": {
        "id": "nfkL0OC1bjcK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.describe()"
      ],
      "metadata": {
        "id": "zuyFudS-bxpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pandas Series have a method called fillna that allows us to replace null values in a column with a value of our choice.\n"
      ],
      "metadata": {
        "id": "zG6yYCTTb1gY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We can make a copy of the dataframe if we don't want to\n",
        "# modify the original (optional)...\n",
        "age_mean_train_df = train_df.copy()\n",
        "# Overwrite the column with new data with the missing data filled\n",
        "age_mean_train_df['Age'] = train_df['Age'].fillna(train_df['Age'].mean())"
      ],
      "metadata": {
        "id": "WhRKlIZHcHby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "age_mean_train_df.describe()"
      ],
      "metadata": {
        "id": "YZwV7Mb8cJvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here's a helpful tip:** if you're curious about the age distribution in the data, you can use the 'value_counts' method with the 'bins' option to group the ages into different ranges. This can give you a sense of how the ages are distributed across the dataset."
      ],
      "metadata": {
        "id": "LPMXx9EscWLF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df[\"Age\"].value_counts(bins=10, sort=False)"
      ],
      "metadata": {
        "id": "1sAI2oebcZ1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Alright, here's a fun exercise for you:\n",
        "\n",
        "Let's include the age of the passengers as an additional feature in our machine learning pipeline.\n",
        "- You can choose whether to use the dataset with the null rows thrown out (e.g., train_df = age_non_null_train_df) or the dataset with the missing age data replaced by the mean (e.g., train_df = age_mean_train_df). It's up to you to decide which approach to take.\n",
        "\n",
        "- Next, you can pick any classifier algorithm you like, such as DecisionTreeClassifier or RandomForestClassifier.\n",
        "- Feel free to experiment with different keyword arguments that you think might make a difference. Try changing them and see if they have any effect on the results.\n",
        "\n",
        "It's a great way to explore and understand the impact of different settings on the model's performance. Have fun tinkering with it!"
      ],
      "metadata": {
        "id": "f6HRFOaacaez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here ..."
      ],
      "metadata": {
        "id": "gs6G-FP7c6_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PRINT SOLUTION (copy/paste output into a cell to run)\n",
        "# (one possible solution ...)\n",
        "show_solution('titanic-age-dropna.py')"
      ],
      "metadata": {
        "id": "gMs9uVO3c8_q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Alright, here's an exciting exercise for you:\n",
        "\n",
        "Let's use the most recently trained model to predict the survival of yourself (or your entire family, if you'd like!).\n",
        "\n",
        "Here's an example to get you started:"
      ],
      "metadata": {
        "id": "P7IOxr_7dKB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You may need to double-check the order of features, it should look like:\n",
        "# [\"Pclass\", \"SibSp\", \"Parch\", \"Age\", \"Sex_male\"]\n",
        "print(X_train.columns)\n",
        "features = X_train.columns\n",
        "\n",
        "family = [\n",
        "  [2, 1, 1, 53.0, 1],  # Me\n",
        "  [2, 1, 1, 52.0, 0],  # Wife\n",
        "  [2, 0, 2, 10.0, 0]   # Daughter\n",
        "]\n",
        "\n",
        "family_df = pd.DataFrame(family, columns=features)\n",
        "model.predict(family_df)"
      ],
      "metadata": {
        "id": "zXb3460hdSY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feel free to modify the 'family' list with the information of your own family members or even predict your own survival individually. The model will predict the survival outcome based on the provided data. Have fun running your predictions!"
      ],
      "metadata": {
        "id": "Fq9S3ATYdTZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Overfitting**\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=1-eRXmB0bTSAvNVYoBeP3P3Ki95IpBaYK' width=200px align=\"right\">\n",
        "\n",
        "Overfitting is something to be mindful of in machine learning. It happens when a model becomes overly specialized to the given data, performing well on that specific data but struggling with new, unseen data. This can occur when the model has an excessive number of adjustable parameters, surpassing what is actually needed for optimal performance.\n",
        "\n",
        "From Wikipedia: `*The green line represents an overfitted model and the black line represents a regularized model. While the green line best follows the training data, it is too dependent on that data and it is likely to have a higher error rate on new unseen data, compared to the black line.*`"
      ],
      "metadata": {
        "id": "dnFudOM7dkBh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " [Here](https://towardsdatascience.com/precision-and-recall-88a3776c8007) is a great analogy to explain Precision and Recall, and I've paraphrased it below:\n",
        "\n",
        "Think of it like fishing with a net. If you cast a wide net into a lake and catch 80 out of 100 fish, that's 80% recall. However, you also end up with 80 rocks in your net, which means your precision is 50% since half of the net's contents are unwanted junk.\n",
        "\n",
        "On the other hand, you could use a smaller net and focus on a specific area of the lake where there are lots of fish and no rocks. In this case, you might only catch 20 out of the fish, but you'll have zero rocks. This results in 20% recall and 100% precision. *italicized text*"
      ],
      "metadata": {
        "id": "M4VKmR-ngZ1P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://drive.google.com/uc?export=view&id=1hR6BOW_L-3vXt_VTYqG48SfstZHqcesh' width=300px align=\"right\">\n",
        "\n",
        "<img src='https://drive.google.com/uc?export=view&id=18W2wnGFpAbAnKOI1I5aPrpXjx89L4lFI' width=300px align=\"left\">\n",
        "\n"
      ],
      "metadata": {
        "id": "DEhCEUjwjQs-"
      }
    }
  ]
}